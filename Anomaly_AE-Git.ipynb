{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b681c3d5-a0c9-45c7-8f45-160ad916b2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cec0f0c-2bf3-4724-8b7e-b84086a8537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the provided Excel file\n",
    "file_path = '/Users/veer/Desktop/RA-Projects/Completed Accuracy calculation/Agriculture building/Normal water consumption.xlsx'  # Adjust the file path as necessary\n",
    "data = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b252135a-0d1d-47e6-92b8-752c49a85422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Date' column to datetime and round to nearest hour\n",
    "data['Date'] = pd.to_datetime(data['Date']).dt.round('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47bd8484-d623-4501-84c2-b81469ebbee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lag features for the past 24 hours\n",
    "for i in range(1, 25):\n",
    "    data[f'Hour_{i}'] = data['Water Consumption(GPM)'].shift(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf81099-231f-4fbd-814d-531360c1ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolating the 'Water Consumption(GPM)' column\n",
    "data['Water Consumption(GPM)'] = data['Water Consumption(GPM)'].interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4760f24e-61ef-45b0-8aec-727698602363",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e6bb44f-5e92-4983-8037-92dbe04c062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "137f0eb8-b7e6-4aa5-8724-7a2e700fd5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 581us/step - loss: 0.1244 - val_loss: 0.0091\n",
      "Epoch 2/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 0.0074 - val_loss: 0.0058\n",
      "Epoch 3/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343us/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 4/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338us/step - loss: 0.0055 - val_loss: 0.0056\n",
      "Epoch 5/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343us/step - loss: 0.0054 - val_loss: 0.0055\n",
      "Epoch 6/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342us/step - loss: 0.0053 - val_loss: 0.0051\n",
      "Epoch 7/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333us/step - loss: 0.0048 - val_loss: 0.0044\n",
      "Epoch 8/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321us/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 9/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335us/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 10/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334us/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 11/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340us/step - loss: 0.0038 - val_loss: 0.0041\n",
      "Epoch 12/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338us/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 13/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329us/step - loss: 0.0038 - val_loss: 0.0041\n",
      "Epoch 14/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323us/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 15/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340us/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 16/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step - loss: 0.0038 - val_loss: 0.0040\n",
      "Epoch 17/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - loss: 0.0039 - val_loss: 0.0040\n",
      "Epoch 18/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343us/step - loss: 0.0038 - val_loss: 0.0040\n",
      "Epoch 19/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336us/step - loss: 0.0038 - val_loss: 0.0040\n",
      "Epoch 20/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step - loss: 0.0038 - val_loss: 0.0040\n",
      "Epoch 21/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336us/step - loss: 0.0039 - val_loss: 0.0040\n",
      "Epoch 22/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337us/step - loss: 0.0038 - val_loss: 0.0040\n",
      "Epoch 23/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - loss: 0.0038 - val_loss: 0.0039\n",
      "Epoch 24/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350us/step - loss: 0.0038 - val_loss: 0.0039\n",
      "Epoch 25/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346us/step - loss: 0.0037 - val_loss: 0.0039\n",
      "Epoch 26/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363us/step - loss: 0.0037 - val_loss: 0.0039\n",
      "Epoch 27/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334us/step - loss: 0.0038 - val_loss: 0.0039\n",
      "Epoch 28/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step - loss: 0.0037 - val_loss: 0.0039\n",
      "Epoch 29/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330us/step - loss: 0.0037 - val_loss: 0.0039\n",
      "Epoch 30/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357us/step - loss: 0.0037 - val_loss: 0.0039\n",
      "Epoch 31/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 386us/step - loss: 0.0037 - val_loss: 0.0039\n",
      "Epoch 32/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - loss: 0.0037 - val_loss: 0.0038\n",
      "Epoch 33/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340us/step - loss: 0.0035 - val_loss: 0.0038\n",
      "Epoch 34/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 0.0036 - val_loss: 0.0038\n",
      "Epoch 35/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 0.0037 - val_loss: 0.0038\n",
      "Epoch 36/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340us/step - loss: 0.0036 - val_loss: 0.0038\n",
      "Epoch 37/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344us/step - loss: 0.0036 - val_loss: 0.0038\n",
      "Epoch 38/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335us/step - loss: 0.0036 - val_loss: 0.0038\n",
      "Epoch 39/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337us/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 40/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342us/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 41/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360us/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 42/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370us/step - loss: 0.0034 - val_loss: 0.0036\n",
      "Epoch 43/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395us/step - loss: 0.0033 - val_loss: 0.0035\n",
      "Epoch 44/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 0.0033 - val_loss: 0.0034\n",
      "Epoch 45/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 46/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 47/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373us/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 48/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - loss: 0.0029 - val_loss: 0.0031\n",
      "Epoch 49/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 371us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 50/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 51/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 52/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 53/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360us/step - loss: 0.0027 - val_loss: 0.0029\n",
      "Epoch 54/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - loss: 0.0027 - val_loss: 0.0029\n",
      "Epoch 55/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369us/step - loss: 0.0026 - val_loss: 0.0029\n",
      "Epoch 56/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 57/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 58/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 59/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332us/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 60/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337us/step - loss: 0.0026 - val_loss: 0.0028\n",
      "Epoch 61/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334us/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 62/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340us/step - loss: 0.0026 - val_loss: 0.0028\n",
      "Epoch 63/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step - loss: 0.0026 - val_loss: 0.0028\n",
      "Epoch 64/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329us/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 65/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331us/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 66/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342us/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 67/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338us/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 68/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334us/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 69/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 70/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 71/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332us/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 72/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327us/step - loss: 0.0025 - val_loss: 0.0027\n",
      "Epoch 73/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325us/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 74/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330us/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 75/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325us/step - loss: 0.0024 - val_loss: 0.0026\n",
      "Epoch 76/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333us/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 77/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333us/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 78/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330us/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 79/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317us/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 80/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318us/step - loss: 0.0024 - val_loss: 0.0026\n",
      "Epoch 81/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312us/step - loss: 0.0024 - val_loss: 0.0026\n",
      "Epoch 82/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370us/step - loss: 0.0024 - val_loss: 0.0026\n",
      "Epoch 83/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326us/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 84/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - loss: 0.0024 - val_loss: 0.0026\n",
      "Epoch 85/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 86/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312us/step - loss: 0.0023 - val_loss: 0.0025\n",
      "Epoch 87/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478us/step - loss: 0.0023 - val_loss: 0.0025\n",
      "Epoch 88/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 412us/step - loss: 0.0023 - val_loss: 0.0025\n",
      "Epoch 89/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330us/step - loss: 0.0023 - val_loss: 0.0025\n",
      "Epoch 90/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316us/step - loss: 0.0023 - val_loss: 0.0025\n",
      "Epoch 91/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 92/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317us/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 93/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 0.0023 - val_loss: 0.0025\n",
      "Epoch 94/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318us/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 95/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315us/step - loss: 0.0023 - val_loss: 0.0025\n",
      "Epoch 96/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - loss: 0.0023 - val_loss: 0.0025\n",
      "Epoch 97/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - loss: 0.0022 - val_loss: 0.0025\n",
      "Epoch 98/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318us/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 99/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327us/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 100/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325us/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 101/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323us/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 102/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322us/step - loss: 0.0023 - val_loss: 0.0025\n",
      "Epoch 103/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329us/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 104/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343us/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 105/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315us/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 106/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318us/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 107/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319us/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 108/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319us/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 109/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313us/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 110/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319us/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 111/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320us/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 112/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309us/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 113/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311us/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 114/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316us/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 115/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321us/step - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 116/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334us/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 117/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317us/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 118/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315us/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 119/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316us/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 120/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328us/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 121/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329us/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 122/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315us/step - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 123/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318us/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 124/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329us/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 125/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343us/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 126/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353us/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 127/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344us/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 128/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327us/step - loss: 0.0020 - val_loss: 0.0022\n",
      "Epoch 129/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325us/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 130/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328us/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 131/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336us/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 132/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 133/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 134/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 135/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 136/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 137/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 138/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327us/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 139/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 140/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 141/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 142/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 143/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 144/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 145/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 146/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 147/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 148/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 149/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 150/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 151/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 152/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 153/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 154/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 155/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 156/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 157/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 158/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 159/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 160/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 161/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 162/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 163/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 164/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 165/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 166/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 167/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 168/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 169/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 170/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 171/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 172/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330us/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 173/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 174/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 175/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 176/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 177/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 178/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 179/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322us/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 180/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 181/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 182/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 183/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 184/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 185/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 186/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 187/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 188/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 189/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 190/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 191/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 192/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 193/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 194/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 195/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 196/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 197/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 198/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 199/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362us/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 200/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 201/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 202/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 203/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 386us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 204/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 371us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 205/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 206/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 207/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 208/500\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187us/step\n",
      "Reconstruction error threshold: 0.008811277678469192\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming 'data' is your DataFrame and it's ready to be used\n",
    "features = [f'Hour_{i}' for i in range(1, 25)]  # The features to be used\n",
    "X = data[features].values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor the validation loss\n",
    "    patience=10,         # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Autoencoder architecture\n",
    "input_dim = X_train.shape[1]\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "encoder = Dense(16, activation=\"relu\")(input_layer)\n",
    "encoder = Dense(8, activation=\"relu\")(encoder)\n",
    "encoder = Dense(4, activation=\"relu\")(encoder)\n",
    "decoder = Dense(8, activation=\"relu\")(encoder)\n",
    "decoder = Dense(16, activation=\"relu\")(decoder)\n",
    "decoder = Dense(input_dim, activation='sigmoid')(decoder)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder with early stopping\n",
    "autoencoder.fit(\n",
    "    X_train, \n",
    "    X_train, \n",
    "    epochs=500,  # You can set a higher number since early stopping will likely stop the training earlier\n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    validation_data=(X_test, X_test),\n",
    "    callbacks=[early_stopping]  # Include the early stopping callback here\n",
    ")\n",
    "\n",
    "# After training, early stopping might have stopped the training at an optimal point\n",
    "\n",
    "\n",
    "# Predict on the training set\n",
    "X_train_pred = autoencoder.predict(X_train)\n",
    "train_mse = np.mean(np.power(X_train - X_train_pred, 2), axis=1)\n",
    "\n",
    "# Try lowering the percentile to 95th or 90th to see if it improves anomaly detection\n",
    "threshold = np.percentile(train_mse, 95)\n",
    "#threshold=0.0035565515548837113\n",
    "\n",
    "print(f\"Reconstruction error threshold: {threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a88eefa-d14d-462a-ae37-fd50414e839a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data rows after preprocessing: 8760\n",
      "Data rows after dropping NaNs: 8644\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188us/step\n",
      "Anomaly Detected at Indices: [4032 4033 4034 4035 4036 4037 4038 4039 4040 4041 4042 4043 4044 4045\n",
      " 4046 4047 4048 4049 4050 4051 4052 4053 4054 4055 4056 4057 4058 4059\n",
      " 4060 4061 4062 4063 4064 4131 4195 4196 4197 4198 4199 4200 4201 4202\n",
      " 4203 4204 4205 4206 4207 4208 4209 4210 4211 4212 4213 4214 4215 4216\n",
      " 4217 4218 4223 4226 4227 4237 4238 4239 4240 4241 4242 4247 4248 4249\n",
      " 4250 4251 4252 4253 4254 4255 4256 4257 4258 4260 4261 4262 4263 4264\n",
      " 4265 4266 4267 4268 4269 4270 4271 4272 4273 4274 4275 4276 4277 4278\n",
      " 4279 4280 4281 4283 4284 4285 4286 4287 4288 4289 4290 4291 4292 4293\n",
      " 4296 4297 4298 4299 4300 4301 4302 4303 4304 4305 4307 4308 4309 4310\n",
      " 4311 4312 4313 4314 4315 4316 4317 4320 4321 4322 4323 4324 4325 4326\n",
      " 4327 4328 4329 4332 4333 4334 4335 4336 4337 4338 4339 4340 4344 4345\n",
      " 4346 4347 4348 4349 4350 4351 4352 4353 4355 4356 4357 4358 4359 4360\n",
      " 4361 4362 4363 4364 4365 4368 4369 4370 4371 4372 4373 4374 4375 4376\n",
      " 4377 4380 4381 4382 4383 4384 4385 4391 4392 4393 4394 4395 4396 4397\n",
      " 4398 4399 4405 4406 4407 4408 4409 4416 4417 4418 4419 4420 4421 4422\n",
      " 4423 4428 4429 4430 4431 4432 4433 4434 4436 4438 4440 4441 4442 4443\n",
      " 4444 4445 4446 4452 4453 4454 4455 4456 4457 4458 4459 4460 4464 4465\n",
      " 4466 4467 4468 4469 4470 4471 4472 4473 4476 4477 4478 4479 4480 4481\n",
      " 4482 4483 4484 4485 4488 4489 4490 4491 4492 4493 4494 4495 4496 4497\n",
      " 4500 4501 4502 4503 4504 4505 4506 4511 4512 4513 4514 4515 4516 4517\n",
      " 4518 4519 4523 4524 4525 4526 4527 4528 4529 4530 4531 4532 4535 4536\n",
      " 4537 4538 4539 4540 4541 4542 4543 4544 4548 4549 4550 4551 4552 4553\n",
      " 4554 4558 4560 4561 4562 4563 4564 4565 4566 4567 4568 4572 4573 4574\n",
      " 4575 4576 4577 4578 4579 4580 4581 4582 4583 4584 4585 4586 4587 4588\n",
      " 4589 4590 4591 4592 4596 4597 4598 4599 4600 4601 4602 4603 4604 4608\n",
      " 4609 4610 4611 4612 4613 4614 4615 4616 4617 4619 4620 4621 4622 4623\n",
      " 4624 4625 4626 4627 4628 4629 4631 4632 4633 4634 4635 4636 4637 4638\n",
      " 4639 4640 4641 4643 4644 4645 4646 4647 4648 4649 4650 4651 4652 4653\n",
      " 4656 4657 4658 4659 4660 4661 4662 4663 4664 4665 4668 4669 4670 4671\n",
      " 4672 4673 4674 4675 4676 4737 4738 4739 4740 4741 4742 4743 4744 4745\n",
      " 4746 4747 4748 4752 4753 4754 4759 4760 4761 8342 8343 8344 8345 8346\n",
      " 8347 8348 8349 8350 8351 8352 8353 8354 8355 8356 8357 8358 8359 8360\n",
      " 8361 8362 8363 8364 8365 8366]\n",
      "Number of Anomalies Detected: 454\n",
      "Length of anomalies: 8644\n",
      "Length of X_new_test: 8644\n",
      "Accuracy: 0.89137, F1: 0.00213, Precision: 0.00220, Recall: 0.00205\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the new dataset\n",
    "new_data_path = '/Users/veer/Desktop/RA-Projects/Completed Accuracy calculation/Agriculture building/+8_ water consumption.xlsx'\n",
    "new_data = pd.read_excel(new_data_path)\n",
    "\n",
    "# Assume the same preprocessing steps\n",
    "new_data['Date'] = pd.to_datetime(new_data['Date']).dt.round('h')\n",
    "for i in range(1, 25):\n",
    "    new_data[f'lag_{i}'] = new_data['Water Consumption(GPM)'].shift(i)\n",
    "\n",
    "# Proper way to apply interpolation without using inplace=True\n",
    "new_data['Water Consumption(GPM)'] = new_data['Water Consumption(GPM)'].interpolate(method='linear')\n",
    "new_data.ffill()\n",
    "new_data.bfill()\n",
    "\n",
    "# Check the number of rows after preprocessing\n",
    "print(\"Data rows after preprocessing:\", new_data.shape[0])\n",
    "\n",
    "# Drop rows with NaN values that resulted from lag feature creation\n",
    "new_data_cleaned = new_data.dropna()\n",
    "print(\"Data rows after dropping NaNs:\", new_data_cleaned.shape[0])\n",
    "\n",
    "# Extract features and normalize\n",
    "features = [f'lag_{i}' for i in range(1, 25)]\n",
    "scaler = MinMaxScaler()\n",
    "X_new = scaler.fit_transform(new_data_cleaned[features])\n",
    "X_new_test = new_data_cleaned['Anomalies'].astype(int)\n",
    "\n",
    "# Predict on the new dataset\n",
    "X_new_pred = autoencoder.predict(X_new)\n",
    "\n",
    "# Calculate the Mean Squared Error (MSE) for each prediction\n",
    "new_mse = np.mean(np.power(X_new - X_new_pred, 2), axis=1)\n",
    "\n",
    "# Detect anomalies by checking where MSE exceeds the threshold\n",
    "anomalies = (new_mse > threshold).astype(int)\n",
    "\n",
    "# Display the results\n",
    "print(\"Anomaly Detected at Indices:\", np.where(anomalies == 1)[0])\n",
    "print(\"Number of Anomalies Detected:\", np.sum(anomalies))\n",
    "\n",
    "# Ensure the length of anomalies and X_new_test are the same\n",
    "print(\"Length of anomalies:\", len(anomalies))\n",
    "print(\"Length of X_new_test:\", len(X_new_test))\n",
    "\n",
    "# Calculate metrics\n",
    "f1 = f1_score(X_new_test, anomalies)\n",
    "precision = precision_score(X_new_test, anomalies)\n",
    "recall = recall_score(X_new_test, anomalies)\n",
    "accuracy = accuracy_score(X_new_test, anomalies)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.5f}, F1: {f1:.5f}, Precision: {precision:.5f}, Recall: {recall:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857e421a-081b-441f-a183-79d0ebdb057c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
